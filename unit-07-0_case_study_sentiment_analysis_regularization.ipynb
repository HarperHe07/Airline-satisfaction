{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1>Unit 7</h1> </center>\n",
    "<center> <h1>Case Study: Sentiment Analysis</h1> </center>\n",
    "<center><h3>(notebook \"sentiment_analysis.ipynb\")</h3></center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 – Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>\n",
    "<center> <h3>http://acuna.io</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives:\n",
    "\n",
    "- The Sentiment Analysis problem.  \n",
    "\n",
    "- A simple solution without data science.  \n",
    "\n",
    "- Application to IMDB reviews.  \n",
    "\n",
    "- Regularized logistic regression.  \n",
    "\n",
    "- Data-driven word sentiments.  \n",
    "\n",
    "- Predicting sentiments in Tweets.  \n",
    "\n",
    "- Spark ML demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Sentiment Analysis problem\n",
    "- Important information about products and services are only expressed through text.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar1.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Sentiment Analysis problem (2)\n",
    "<br>\n",
    "<sup>Tumasjan, Andranik; O.Sprenger, Timm; G.Sandner, Philipp; M.Welpe, Isabell (2010). \"**Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment**\". \"Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.\"</sup>  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar2.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Sentiment Analysis problem (3)\n",
    "- From Wikipedia:\n",
    "<div class=\"blockquote2\">\n",
    "  <p>Sentiment analysis refers to the use of natural language processing, text analysis and computational linguistics to identify and extract **subjective information** in source materials.</p>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A simple solution without much data science\n",
    "- Get a list of positive words (+1) and negative words (-1).\n",
    "- Compute the mean sentiment in the text based on words.\n",
    "- If mean is greater than 0, then sentence is positive.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar3.png\" width=\"90%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Application to IMDB\n",
    "- IMDB  (The Internet Movie Database) contains volunteers' reviews of movies.  \n",
    "\n",
    "- We will be using a dataset of 12.5K positive reviews and 12.5K negative reviews.  \n",
    "\n",
    "- We assume, reviews (from 1-10) with greater than 7 had positive sentiment and less than 4 had negative sentiment.  \n",
    "\n",
    "<br>\n",
    "<sup>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf). The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).</sup>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Application to IMDB (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar4.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tagging based on list of word sentiments\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar5.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple model performance\n",
    "- Predicting sentiment based on simple word sentiment averaging:\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar6.png\" width=\"20%\" align=\"center\"></center>\n",
    "<br>\n",
    "\n",
    "- Measuring performance with accuracy is appropriate since dataset is balanced.  \n",
    "\n",
    "- Pretty good considering that human-human correlation of rating is around 75%!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with simple approach\n",
    "- Positive and negative words have the same weight (e.g., good == amazing.)  \n",
    "\n",
    "- Maybe a couple of negative words make the entire review negative, whereas positive words do not.  \n",
    "\n",
    "- While our dataset is artificially balanced (12500 positive and 12500 negative), there are usually more positive than negative reviews, and therefore we should bias our predictions towards positive ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A data science approach\n",
    "- Idea: use words in review to *predict* sentiment.  \n",
    "\n",
    "- Using supervised data:\n",
    "  - For a review \"I am going to sue this hotel for how bad WiFi is\" learn a function $f$ such that  \n",
    "  \n",
    "$$p(\\text{negative} \\mid \\text{sentence}) = f(\\theta_0 + \\theta_\\text{sue} + \\theta_\\text{hotel} + \\theta_\\text{bad} + \\theta_\\text{wifi})$$\n",
    "\n",
    "- We are considering the average review with $\\theta_0$.  \n",
    "\n",
    "- We throw away words that \"do not matter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From text to vectors\n",
    "- We need to represent reviews as vectors so that we can estimate their weights.\n",
    "\n",
    "- We should consider how many times a word is mentioned:  \n",
    "\n",
    "  \"This hotel is super super super good\" vs \"This hotel is super good.\"  \n",
    "\n",
    "- There are many mechanisms for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Consider the following documents\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar7.png\" width=\"40%\" align\n",
    "             =\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word counting\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar8.png\" width=\"90%\" align=\"center\"></center>\n",
    "<br>\n",
    "- It inflates common words: \"are\" would be counted as much as \"cats\" and \"dogs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Term-frequency inverse document frequency\n",
    "- Words common to all documents get discounted by the inverse of their frequency.  \n",
    "\n",
    "- Raw frequencies get discounted as well:\n",
    "  \n",
    "  \"super super super good\" almost the same as \"super super good\"\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar9.png\" width=\"50%\" align=\"center\"></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Term-frequency inverse document frequency (2)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar10.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression\n",
    "- Since sentiments are binary, we can use a classification approach known as logistic regression.  \n",
    "\n",
    "- In logistic regression, we want to maximize the probability of our observed positive and negative examples with the following form:\n",
    "\n",
    "$$p(y \\mid X) = p_\\theta(X)^y(1-p_\\theta(X))^{1-y}$$  \n",
    "\n",
    "\n",
    "$\\qquad$where $p_\\theta(X)$ is the sigmoid ($\\sigma$) function:\n",
    "\n",
    "$$p_\\theta(X) = \\frac{1}{1 + \\exp(-(\\theta_0 + \\sum_{j>0} x_j\\theta_j))}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression: loss function\n",
    "- We transform the reviews into TF-IDF vectors X.  \n",
    "\n",
    "- Using positive (y=1) and negative examples (y=0), derive the loss function from:\n",
    "\n",
    "$$p(y \\mid X) = p_\\theta(X)^y(1-p_\\theta(X))^{1-y}$$  \n",
    "\n",
    "\n",
    "$\\qquad$and\n",
    "\n",
    "$$p_\\theta(X) = \\frac{1}{1 + \\exp(-(\\theta_0 + \\sum_{j>0} x_j\\theta_j))}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression to estimate sentiments\n",
    "- Splits: training (60%), validation (30%), and testing (10%).  \n",
    "\n",
    "- The loss function is:\n",
    "\n",
    "$$L_\\theta(X,Y) = -\\left(\\sum_i Y_i\\log p_\\theta(X_i) + (1-Y_i)\\log (1-p_\\theta(X_i))\\right)$$\n",
    "- Performance on validation set:\n",
    "\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar11.png\" width=\"20%\" align=\"center\"></center>\n",
    "\n",
    "- Much better than simple approach (73% accuracy.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examining the fit\n",
    "- Logistic regression estimated the weight of each word ($𝜃_\\text{word}$).  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar12.png\" width=\"80%\" align=\"center\"></center>\n",
    "\n",
    "- Therefore, we can infer which words are \"negative\" and which are \"positive\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examining the fit (2)\n",
    "- Most negative words according to logistic regression.  \n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar13.png\" width=\"40%\" align=\"center\"></center>\n",
    "- Most positive words:  \n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar14.png\" width=\"40%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Something is off: overfitting\n",
    "- Notice that there are 290K+ words in the dataset but only 25K reviews are used for training.  \n",
    "\n",
    "- #features > #training examples leads to potential overfitting.  \n",
    "\n",
    "- We could try to select features by searching but it would take too long for 290K features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularized logistic regression\n",
    "- We can instead create a loss function that *penalizes learning*.  \n",
    "\n",
    "- This approach is called *regularization*.  \n",
    "\n",
    "- There are two main regularization methods: **L2** (or **ridge**) and **L1** (or **lasso**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L2 regularization\n",
    "- We build an alternative loss function where the weights of the parameters quadratically increase the loss.  \n",
    "\n",
    "$$L_{\\theta}^\\lambda(X,Y) = -\\left(\\sum_i Y_i \\log p_\\theta(X_i) + (1-Y_i)\\log(1-p_\\theta(X_i))\\right) + \\lambda\\sum_{j>0}\\theta_j^2$$\n",
    "\n",
    "- **We will think twice about increasing the importance of a word in the prediction unless it fits the data well**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# L1 regularization\n",
    "- One problem with L2 regularization is that all *features matter* even if by a *little*.  \n",
    "\n",
    "- This is undesirable because intuitively some features do not matter (weight = 0.)  \n",
    "\n",
    "- L1 solves this by imposing the following loss function:  \n",
    "\n",
    "$$L_{\\theta}^\\lambda(X,Y) = -\\left(\\sum_i Y_i \\log p_\\theta(X_i) + (1-Y_i)\\log(1-p_\\theta(X_i))\\right) + \\lambda\\sum_{j>0}\\mid\\theta_j\\mid$$\n",
    "\n",
    "- We penalize learning by the absolute value.  \n",
    "\n",
    "- **The practical effect is that some features will have zero weight**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Elastic net logistic regularization\n",
    "- One problem with L1 regularization is that it cannot choose more features than training examples.\n",
    "- We would like to have a combination of L1 and L2.\n",
    "- This is called **Elastic Net Regularization**:\n",
    "\n",
    "$$L_{\\theta}^{\\lambda , \\alpha}(X,Y) = -\\left(\\sum_i Y_i \\log p_\\theta(X_i) + (1-Y_i)\\log(1-p_\\theta(X_i))\\right) + \\lambda\\left((1-\\alpha)\\sum_{j>0}\\theta_j^2 + \\alpha\\sum_{j>0}\\mid\\theta_j\\mid\\right)$$\n",
    "\n",
    "- $\\alpha$ controls how much importance is given to L1 vs L2.\n",
    "- We now have a family of models controlled by $\\alpha$ and $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Elastic net logistic regression (2)\n",
    "- We apply elastic net logistic regression to sentiment prediction using $\\alpha$=0.3 (30% L1, 70% L2) and $\\lambda$=0.02.  \n",
    "\n",
    "- Performance on validation set:\n",
    "\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar15.png\" width=\"50%\" align=\"center\"></center>\n",
    "\n",
    "- We improved performance from simple logistic regression (83%)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word-level sentiments\n",
    "- The most negative words:\n",
    "\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar16.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word-level sentiments (2)\n",
    "- The most positive words:\n",
    "\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar17.png\" width=\"20%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic feature selection with L1\n",
    "- This model has 25554 features with exactly zero weight! (95% of features.)\n",
    "\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar18.png\" width=\"17%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How we choose $\\alpha$ and $\\lambda$?\n",
    "- To be more formal, we should run an experiment through a grid of $\\alpha$ and $\\lambda$ values.\n",
    "- We run $\\lambda \\in \\{0, 0.01, 0.02\\}$ and $\\alpha \\in \\{0, 0.2, 0.4\\}$\n",
    "- We found the best parameter are $\\alpha=0.2$ and $\\lambda=0.01$.\n",
    "- Final, accuracy:\n",
    "\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar19.png\" width=\"25%\" align=\"center\"></center>\n",
    "\n",
    "- So we went from 73%, to 83% (unregularized), 86% (regularized), to 87% (regularized grid search.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How should we report the final performance of our case?\n",
    "- We have used the training dataset to fit each model.  \n",
    "\n",
    "- We have used the validation dataset to compare across models.  \n",
    "\n",
    "- But we should report, at the end, the performance of our best model on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examining problems with the model\n",
    "- Logistic regression produces a probability that an example belongs to a class.  \n",
    "\n",
    "- With this, we can examine the places where our model is **most wrong** in the supervised data.  \n",
    "\n",
    "- Also, we can examine where our model is **most confident** about a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying it to Twitter data\n",
    "- Applying our model to a sample of 1K tweets from Clinton and Trump, during the 2016 US presidential campaign.  \n",
    "\n",
    "- A sample of Trump’s tweets that are negative (with high confidence):  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar20.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying it to Twitter data (2)\n",
    "- A sample of Clinton’s tweets that are negative (with high confidence):  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar21.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying it to Twitter data (3)\n",
    "- Positive Trump's tweets:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar22.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying it to Twitter data (4)\n",
    "- Positive Clinton's tweets:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar23.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying it to Twitter data (5)\n",
    "- And the verdict is:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-07/unit-07-0_cssar24.png\" width=\"70%\" align=\"center\"></center>  \n",
    "\n",
    "- Clinton is apparently significantly more negative than Trump!\n",
    "- Any ideas why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "- With supervised data, we can apply data science to model the sentiment of words and whole texts.  \n",
    "\n",
    "- Regularized logistic regression allows to:\n",
    "  1. select variables, \n",
    "  2. avoid overfitting.  \n",
    "  \n",
    "- Spark ML has pipelines to deal with big data science jobs, from cleaning up raw data to fitting complex statistical models."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
